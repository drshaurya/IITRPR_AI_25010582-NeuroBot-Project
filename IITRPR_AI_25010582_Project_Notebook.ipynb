{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Definition & Objective\n",
        "Selected Project Track: AI in Personalized Learning\n",
        "\n",
        "I have built an algorithm which allows the user (medical student) to attempt questions related to neurology, provides explanations for incorrect answers, uses reinforcement learning to present relevant questions to the user, and eventually provides an analysis of which topics the student needs to work on.\n",
        "\n",
        "\n",
        "Neurology is a branch of medicine that deals with diseases of the human nervous system, including the brain, spinal cord and nerves. Medical students often find it hard to learn and memorise concepts related to this field, because of the complexity of concepts and confusing terminology. In my personal experience, a Q&A based learning tool is the ideal resource to help students retain information better.\n",
        "\n"
      ],
      "metadata": {
        "id": "F9Tk36S8JbaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding & Preparation\n",
        "Data used for developing this project was synthetically generated using ChatGPT 5.2. The chatbot was initially prompted to generate a dataset of 50 multiple-choice questions from the field of neurology (more specifically, neuroanatomy and its clinical aspects). These questions were accompanied by answers and short explanations about the answers. ChatGPT was told to ensure that the difficulty level of questions was equally distributed between easy, moderate and hard. The data was then expanded to 200 questions, with the condition that the content must include all aspects of neurology in comparable quantity. The final dataset used was in the form of a .csv file, which included columns for question; options a, b, c and d; correct option; topic; difficulty; and explanation."
      ],
      "metadata": {
        "id": "8JOcoX14R5DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading"
      ],
      "metadata": {
        "id": "T1IhMe9HjEh9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ed701e0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "quiz_data = pd.read_csv('/content/ProjectDataSet.csv')\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Exploration\n",
        "\n",
        "Since this data was specifically generated for this project, it is a clean dataset not needing any traditional data cleaning or handling of noise."
      ],
      "metadata": {
        "id": "jWKP7uayjKm5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "814b003f",
        "outputId": "be9bb0a1-0766-4c54-d224-88fcb15e4c56"
      },
      "source": [
        "available_questions = quiz_data.to_dict(orient='records')\n",
        "\n",
        "topic_performance = {}\n",
        "for topic in quiz_data['topic'].unique():\n",
        "    topic_performance[topic] = {'correct_count': 0, 'incorrect_count': 0, 'attempted': 0}\n",
        "\n",
        "print(f\"Initialized {len(available_questions)} questions in available_questions.\")\n",
        "print(f\"Initialized {len(topic_performance)} topics in topic_performance with counts set to zero.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized 200 questions in available_questions.\n",
            "Initialized 39 topics in topic_performance with counts set to zero.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Design\n",
        "\n",
        "Heuristic Optimization, Greedy Policies and Stochastic Decision Processes"
      ],
      "metadata": {
        "id": "LGu7L97RimIS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9068ecd9"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_adaptive_question(available_questions, topic_performance):\n",
        "    if not available_questions:\n",
        "        return None\n",
        "\n",
        "    # Identify weak topics (those with the highest incorrect_count)\n",
        "    weak_topics = []\n",
        "    max_error_rate = -1\n",
        "\n",
        "    for topic, perf in topic_performance.items():\n",
        "        if perf['attempted'] == 0:\n",
        "            continue   # PREVENTS division by zero\n",
        "\n",
        "        error_rate = perf['incorrect_count'] / perf['attempted']\n",
        "\n",
        "        if error_rate > max_error_rate:\n",
        "            max_error_rate = error_rate\n",
        "            weak_topics = [topic]\n",
        "        elif error_rate == max_error_rate:\n",
        "            weak_topics.append(topic)\n",
        "\n",
        "    '''\n",
        "    If all topics have 0 incorrect answers or no incorrect answers recorded yet,\n",
        "    or if max_incorrect_count is still -1 (meaning topic_performance is empty or all counts are 0),\n",
        "    we will treat all topics equally or pick randomly from all available.\n",
        "    '''\n",
        "\n",
        "    # Try to find a question from weak topics first\n",
        "    if weak_topics: # Only prioritize if there are actual incorrect answers\n",
        "        prioritized_questions = [q for q in available_questions if q['topic'] in weak_topics]\n",
        "        if prioritized_questions:\n",
        "            selected_question = random.choice(prioritized_questions)\n",
        "            available_questions.remove(selected_question)\n",
        "            return selected_question\n",
        "\n",
        "    '''\n",
        "    Fallback: If no weak topics (or no questions in them) or all topics are equally 'strong' (0 incorrect answers),\n",
        "    pick a random question from the remaining available questions.\n",
        "    '''\n",
        "    selected_question = random.choice(available_questions)\n",
        "    available_questions.remove(selected_question)\n",
        "    return selected_question"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Justification of design choices\n",
        "1. Capping each session at 50 questions (terminating loops accordingly) - to ensure diversity of knowledge and prevent cognitive fatigue\n",
        "2. Tracking performance for each sub-topic within Neurology - to help target weaker areas rather than repetitively learing everything\n",
        "3. Preferentially selecting questions from weaker topics i.e. those with high error rates - same as above\n",
        "4. Representing topic performance as a ratio (correct / attempted) - to normalize performance assessment across unequally distributed data\n",
        "5. Hybrid logic with deterministic and probabilistic processes - keeps behaviour easy to interpret while not being predictable"
      ],
      "metadata": {
        "id": "wh3DMQAYkt6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Implementation"
      ],
      "metadata": {
        "id": "byG73qaOkA_i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "625b8fae",
        "outputId": "94e78ad1-73cc-4523-8456-5713980683b3"
      },
      "source": [
        "score = 0\n",
        "correct_answers_count = 0\n",
        "incorrect_answers_count = 0\n",
        "question_results = [] # To store detailed results for analysis\n",
        "\n",
        "question_number = 0\n",
        "max_questions = 25\n",
        "while available_questions and question_number < max_questions:\n",
        "    question_number += 1\n",
        "    current_question = get_adaptive_question(available_questions, topic_performance)\n",
        "\n",
        "    if current_question is None:\n",
        "        print(\"No more questions available.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\nQuestion {question_number}: {current_question['question_text']}\")\n",
        "    print(f\"A: {current_question['option_a']}\")\n",
        "    print(f\"B: {current_question['option_b']}\")\n",
        "    print(f\"C: {current_question['option_c']}\")\n",
        "    print(f\"D: {current_question['option_d']}\")\n",
        "\n",
        "    while True:\n",
        "        user_answer = input(\"Your answer (A, B, C, or D): \").strip().lower()\n",
        "        if user_answer in ['a', 'b', 'c', 'd']:\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid input. Please enter A, B, C, or D.\")\n",
        "\n",
        "    correct_answer_char = current_question['correct_option'].lower()\n",
        "    is_correct = (user_answer == correct_answer_char)\n",
        "\n",
        "    # Update topic_performance\n",
        "    question_topic = current_question['topic']\n",
        "    topic_performance[question_topic]['attempted'] += 1\n",
        "    if is_correct:\n",
        "        print(\"Correct!\")\n",
        "        score += 1\n",
        "        correct_answers_count += 1\n",
        "        topic_performance[question_topic]['correct_count'] += 1\n",
        "    else:\n",
        "        print(f\"Incorrect. The correct answer was {correct_answer_char.upper()}.\")\n",
        "        incorrect_answers_count += 1\n",
        "        topic_performance[question_topic]['incorrect_count'] += 1\n",
        "        if 'explanation' in current_question:\n",
        "            print(f\"Explanation: {current_question['explanation']}\")\n",
        "        else:\n",
        "            print(\"Explanation unavailable.\")\n",
        "\n",
        "    question_results.append({\n",
        "        'id': current_question['id'],\n",
        "        'topic': question_topic,\n",
        "        'is_correct': is_correct\n",
        "    })\n",
        "\n",
        "    # Real-time summary after each question\n",
        "    print(\"\\n--- Current Progress ---\")\n",
        "    print(f\"Questions Answered: {question_number}\")\n",
        "    print(f\"Current Score: {score}\")\n",
        "    print(f\"Correct Answers: {correct_answers_count}\")\n",
        "    print(f\"Incorrect Answers: {incorrect_answers_count}\")\n",
        "    current_accuracy = (correct_answers_count / question_number) * 100 if question_number > 0 else 0\n",
        "    print(f\"Current Accuracy: {current_accuracy:.2f}%\")\n",
        "\n",
        "final_total_questions = correct_answers_count + incorrect_answers_count\n",
        "print(f\"\\nQuiz finished! Your final score is: {score}/{final_total_questions}\")\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1: The posterior limb of the internal capsule primarily carries:\n",
            "A: Visual signals\n",
            "B: Motor fibers\n",
            "C: Cerebellar signals\n",
            "D: Pain fibers\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1347096562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0muser_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your answer (A, B, C, or D): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_answer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b472659d"
      },
      "source": [
        "print(\"\\n--- Adaptive Quiz Performance Summary ---\")\n",
        "\n",
        "# Overall Quiz Performance\n",
        "final_total_questions = correct_answers_count + incorrect_answers_count\n",
        "if final_total_questions > 0:\n",
        "    overall_accuracy = (correct_answers_count / final_total_questions) * 100\n",
        "else:\n",
        "    overall_accuracy = 0\n",
        "\n",
        "print(f\"Total Questions Attempted: {final_total_questions}\")\n",
        "print(f\"Overall Correct Answers: {correct_answers_count}\")\n",
        "print(f\"Overall Incorrect Answers: {incorrect_answers_count}\")\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.2f}%\\n\")\n",
        "\n",
        "# Detailed Topic-wise Performance\n",
        "print(\"--- Topic-wise Performance Analysis ---\")\n",
        "\n",
        "topic_accuracies = {}\n",
        "for topic, performance in topic_performance.items():\n",
        "    topic_total = performance['correct_count'] + performance['incorrect_count']\n",
        "    if topic_total > 0:\n",
        "        topic_accuracy = (performance['correct_count'] / topic_total) * 100\n",
        "        topic_accuracies[topic] = {\n",
        "            'total_attempted': topic_total,\n",
        "            'correct': performance['correct_count'],\n",
        "            'incorrect': performance['incorrect_count'],\n",
        "            'accuracy': topic_accuracy\n",
        "        }\n",
        "        print(f\"Topic: {topic}\")\n",
        "        print(f\"  Questions Attempted: {topic_total}\")\n",
        "        print(f\"  Correct: {performance['correct_count']}\")\n",
        "        print(f\"  Incorrect: {performance['incorrect_count']}\")\n",
        "        print(f\"  Accuracy: {topic_accuracy:.2f}%\\n\")\n",
        "    else:\n",
        "        # Only print topics that had questions attempted during the quiz\n",
        "        if performance['correct_count'] > 0 or performance['incorrect_count'] > 0:\n",
        "            print(f\"Topic: {topic} (No questions attempted or issues in recording performance)\\n\")\n",
        "\n",
        "# Identify Weakest Topics\n",
        "if topic_accuracies:\n",
        "    sorted_topics = sorted(topic_accuracies.items(), key=lambda item: item[1]['accuracy'])\n",
        "\n",
        "    print(\"--- Weakest Topics (by accuracy) ---\")\n",
        "    # Print up to 5 weakest topics, or fewer if not many topics were attempted\n",
        "    for i, (topic, data) in enumerate(sorted_topics):\n",
        "        if i >= 5: # Limit to top 5 weakest topics\n",
        "            break\n",
        "        if data['total_attempted'] > 0 and data['accuracy'] < 100:\n",
        "            print(f\"- {topic}: {data['incorrect']} incorrect out of {data['total_attempted']} (Accuracy: {data['accuracy']:.2f}%)\")\n",
        "\n",
        "    if not any(data['accuracy'] < 100 and data['total_attempted'] > 0 for data in topic_accuracies.values()):\n",
        "        print(\"Great job! No incorrect answers in any topic.\")\n",
        "else:\n",
        "    print(\"No topic-wise data available for analysis.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Analysis\n",
        "Performance analysis\n",
        "\n",
        "The performance analysis of this adaptive learning model focuses on how effectively it adapts question difficulty and topic selection and how much it improves learning outcomes in neurology. Since the model provides both real-time feedback and comprehensive post-session analysis, performance is assessed across adaptivity, learning gain, and diagnostic capability.\n",
        "\n",
        "Parameters of evaluation\n",
        "1. Efficient question selection: Reflecting the model's ability to personalize, minimise redundancy while maximizing educational value.\n",
        "2. Significant learning gains: Measured as the model's ability to provide adaptive sequencing and feedback mechanisms inorder to actively contribute to improved understanding in place of traditional passive assessment.\n",
        "\n",
        "As this is a basic prototype, currently this is an intutive analysis performed by the developer in the test runs."
      ],
      "metadata": {
        "id": "tyvkVHsejCqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations and Ethical Considerations\n",
        "This tool can be significantly refined to perform optimally. The dataset that I have used to build it currently, has been synthetically generated by ChatGPT and is also a small dataset of 200 questions with answers and explanations. The scope of this project is currently limited only to Neurology but can be expanded to include other specialties of medicine. There are no concerns about the ethical implications of the tool as it does not require any personal data from the user, and the model design does not inherently predispose to bias."
      ],
      "metadata": {
        "id": "WCnkyHjajV5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Future Scope\n",
        "In conclusion, I created a tool to help medical students learn about Neurology and understand what topics they are weak in. Through this process, I improved my understanding of coding and Artificial Intelligence.\n",
        "\n",
        "Future improvements to the tool could include introducing a spaced repetition system for questions answered incorrectly, or provision of learning resources or explanations for topics that appear to be persistently weak, or allowing users to focus on specific weak topics identified in the performance summary.\n",
        "A better user interface (UI) can also be developed, along with a gamification layer, to make the experience more enjoyable for the user."
      ],
      "metadata": {
        "id": "VQ-GMmOwjeBR"
      }
    }
  ]
}